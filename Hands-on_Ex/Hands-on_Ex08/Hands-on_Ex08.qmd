---
title: "Hands-on exercise08"
author: "Yuheng Liang"
format: html
editor: visual
date: "Oct 4, 2024"
date-modified: "Oct 15, 2024"
execute: 
  eval: true
  echo: true
  freeze: true
---

# Hands-on exercise 08-09

## 1.0 The data

### 1.1 Installing and loading R packages

```{r}
pacman::p_load(spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
```

### 1.2 Data Import and Prepatation

Importing geospatial data into R environment

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>%
  select(c(2:7))
```

```{r}
shan_sf
```

glimpse() to reveal the data type of it’s fields

```{r}
glimpse(shan_sf)
```

### 1.3 Importing aspatial data into R environment

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
```

summary statistics of ict data.frame.

```{r}
summary(ict)
```

The unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc. In order to overcome this problem, we will derive the penetration rate of each ICT variable by using the code chunk below.

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
```

review the summary statistics of the newly derived penetration rates

```{r}
summary(ict_derived)
```

Six new fields have been added into the data.frame.

## 2.0 Exploratory Data Analysis (EDA)

### 2.1 EDA using statistical graphics

plot the distribution of the variables by using EDA

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Boxplot is useful to detect if there are outliers.

```{r}
ggplot(data = ict_derived, aes(x=`RADIO`))+
  geom_boxplot(color="black", fill="light blue")
```

plotting the distrubution of the newly derived variables

```{r}
ggplot(data = ict_derived, aes(x=`RADIO_PR`))+
         geom_histogram(bins=20,
                        color="black",
                        fill="light blue")
```

```{r}
ggplot(data = ict_derived,aes(x=`RADIO_PR`))+
  geom_boxplot(color = "black",
               fill="light blue")
```

create the data visualisation First, we will create the individual histograms using the code chunk below.

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Next, the ggarrange() function of ggpubr package is used to group these histograms together.

```{r}
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

### 2.2 EDA using choropleth map

#### 2.2.1 joint the geospatial data with aspatial data

```{r}
shan_sf <- left_join(shan_sf,ict_derived, by=c("TS_PCODE"="TS_PCODE"))
write_rds(shan_sf, "data/rds/shan_sf.rds")
```

```{r}
shan_sf <- read_rds("data/rds/shan_sf.rds")
```

#### 2.2.2 Preparing a choropleth map

To have a quick look at the distribution of Radio penetration rate of Shan State at township level, a choropleth map will be prepared.

```{r}
qtm(shan_sf, "RADIO_PR")
```

Reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships.

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

difference: First Code： Snippet generates two independent maps, allowing for detailed analysis of each variable. This is ideal when you want to focus on a specific variable. Second Code： Snippet presents multiple variables in the same view, making it better for simultaneous observation of related variables.

## 3.0 Correlation Analysis

Before we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

The correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both.

## 4.0 Hierarchy Cluster Analysis

### 4.1 Extracting clustering variables

extract the clustering variables from the shan_sf simple feature object into data.frame

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.

change the rows by township name instead of row number

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

delete the TS.x field by using the code chunk below

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

### 4.2 Data Standardisation

In general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.

### 4.3 Min-Max standardisation

normalize() of heatmaply package: used to stadardisation the clustering variables by using Min-Max method. summary(): used to display the summary statistics of the standardised clustering variables

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

the values range of the Min-max standardised clustering variables are 0-1 now.

### 4.4 Z-score standardisation

Z-score standardisation can be performed easily by using scale() of Base R.

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively. describe() of psych package is used here instead of summary() of Base R because the earlier provides standard deviation. Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.

### 4.5 Visualising the standardised clustering variables

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

Raw values without standardization: The distribution appears right-skewed, with most of the values concentrated between 100 and 300. There are fewer values in the higher range (above 400) Min-Max Standardization: The distribution remains largely the same in terms of shape but is rescaled to the \[0, 1\] range. Z-score Standardization: This transforms the data to have a mean of 0 and a standard deviation of 1. The shape remains right-skewed as well, but the axis has changed to standard deviations from the mean.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

### 4.6 Computing proximity matrix

We will compute the proximity matrix by using dist() of R. dist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix. computr the proximity matrix using edclidean method.

```{r}
proxmat <- dist(shan_ict,method = 'euclidean')
```

list the content of proxmat fot visual inspection

```{r}
proxmat
```

### 4.7 computing hierarchical clustering

hclust() of R stats will be used to hierarchical clustering. hclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC). The code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

plot it

```{r}
plot(hclust_ward,cex =0.6)
```

### 4.8 Selecting the optimal clustering algorithm

compute the agglomerative coefficients of all hierarchical clustering algorithms.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

Ward’s method provides the strongest clustering structure among the four methods assessed Hence, in the subsequent analysis, only Ward’s method will be used.

### 4.9 Determining Optimal Clusters

Gap Statistic Method The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict,
                    FUN = hcut,
                    K.max = 10,
                    B= 50)
print(gap_stat,methods ="firstmax")
```

visualise the plot by using fviz_gap_stat() of factoextra package.

```{r}
fviz_gap_stat(gap_stat)
```

### 4.10 Interpreting the dendrograms

In the dendrogram displayed above, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height. The height of the fusion, provided on the vertical axis, indicates the (dis)similarity between two observations. The higher the height of the fusion, the less similar the observations are. Note that, conclusions about the proximity of two observations can be drawn only based on the height where branches containing those two observations first are fused. We cannot use the proximity of two observations along the horizontal axis as a criteria of their similarity. draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward,
            k=6,
            border = 2:5)
```

### 4.11 Visually-driven hierarchical clustering analysis

#### 4.11.1 Transforming the data frame into a matrix

transform shan_ict data frame into a data matrix.

```{r}
shan_ict_mat <- data.matrix(shan_ict)
```

#### 4.11.2 Plotting interactive cluster heatmap using heatmaply()

the heatmaply() of heatmaply package is used to build an interactive cluster heatmap

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```

### 4.12 Mapping the clusters formed

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

Output is called groups. It is a list object the groups object need to be appended onto shan_sf simple feature object. The code chunk below form the join in three steps: the groups list object will be converted into a matrix; cbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and rename of dplyr package is used to rename as.matrix.groups field as CLUSTER.

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

plot the choropleth map

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```

## 5.0 Spatially Constrained Clustering : SKATER approach

### 5.1 Converting into SpatialPolygonsDataFrame

SKATER function only support sp projects such as SpatialPolygonDataFrame

```{r}
shan_sp <- as_Spatial(shan_sf)
```

### 5.2 Computing Neighbour List

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

plot the neighbours list on Shan_sp by using the code chunk below. The first plot command gives the boundaries. This is follow by the plot of the neighbor list object, with coordinates applied to the original SpatialPolygonDateFrame to extract the centroids of the polygons.

```{r}
coords <- st_coordinates(
  st_centroid(st_geometry(shan_sf))
)
```

plot

```{r}
plot(st_geometry(shan_sf),
     border = grey(.5))
plot(shan.nb,
     coords,
     col = "blue",
     add=TRUE)
```

Note that if you plot the network first and then the boundaries, some of the areas will be clipped. This is because the plotting area is determined by the characteristics of the first plot.

### 5.3 Computing minimun spanning tree

#### 5.3.1 Calculating edge costs

nbcosts() of spdep package is used to compute the cost of each edge. It is the distance between the nodes. This function compute this distance using a data.frame with observations vector in each node.

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

For each observation, this gives the pairwise dissimilarity between its valuse on the five variables and the values for the neighbouring observation. Basically, this is the notion if a generalised weight for a spatial weights matrix.

We will incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.

```{r}
shan.w <- nb2listw(shan.nb,
                   lcosts,
                   style = "B")
summary(shan.w)
```

### 5.4 Computing minimun spanning tree

```{r}
shan.mst <- mstree(shan.w)
```

check its class and dimension by using the code chunk below

```{r}
class(shan.mst)
```

```{r}
dim(shan.mst)
```

the dimension is 54 and not 55. This is because the minimum spanning tree cosistson n-1 edges(links) in order to traverse all the nodes display the content of shan.mst

```{r}
head(shan.mst)
```

The plot method fot the MST include a way to show the observation numbers of the nodes in addition to the edge.

```{r}
plot(st_geometry(shan_sf),
     border = gray(.5))
plot.mst(shan.mst,
         coords,
         col="blue",
         cex.lab=0.7,
         cex.circles = 0.005,
         add = TRUE)
```

### 5.5 Computing spatially constrained clusters using SKATER method

Compute the spatially constrained cluster

```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

The skater() takes three mandatory arguments: The first two columns of the MST matrix The data matrix(to update the costs as units are being grouped) The number of cuts

examine its contents

```{r}
str(clust6)
```

check the cluster assignment

```{r}
ccs6 <- clust6$groups
ccs6
```

we can find out how many observations are in each cluster by means of the table command.we can also find this as the dimension of each vector in the list contained in edge

```{r}
table(ccs6)
```

plot the pruned tree that shows the five cluster on top of the townshop area

```{R}
plot(st_geometry(shan_sf), 
     border=gray(.5))
plot(clust6, 
     coords, 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```

### 5.6 Visualising the clusters in choropleth map

plot the newly derived cluster

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster,as.factor(groups_mat))%>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster,"SP_CLUSTER")
```

place both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```

## 6.0 Spatially Constrained Clustering: ClustGeo Methodx

### 6.1 About ClustGeo package

ClusterGeo is an R package specially designed to support the need of performing spatially constrained cluster analysis. And it also provides a Ward-like hierarchical clustering algorithm called hclustgeo() including spatial/geographical constrains The idea is then to determine a value of alpha which increase the spatial contiguity without deteriorting too much the quality of the solution based on the variables of interest. This need is supported by a function called choicealpha() \### 6.2 Ward-like hierarchical clustering: ClustGeo ClustGeo package provides function called hclusegeo() to perform a typical Ward-likeb hierarchical clustering just like hclust(). To perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix

```{r}
nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster,cex= 0.5)
rect.hclust(nongeo_cluster,
            k = 6,
            border = 2:5)
```

the dissimilarity matrix must be an object of class dist. \#### 6.2.1 Mapping the clister formed

```{r}
groups <- as.factor(cutree(nongeo_cluster,k=6))
```

```{r}
shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
```

```{r}
qtm(shan_sf_ngeo_cluster,"CLUSTER")
```

### 6.3 Spatially Constrained Hierarchical Clustering

Before we can performed spatially constrained hierarchiacl clustering, a spatial distance matrix will be derived by using st_distance() of sf package.

```{r}
dist<- st_distance(shan_sf,shan_sf)
distmat <- as.dist(dist)
```

as.dist is used to convert the data frame into matrix choicealpha() will be used to determine a suitable value for the mixing parameter alpha as shown in the code chunk below.

```{r}
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```

alpha = 0.2

```{r}
clustG <- hclustgeo(proxmat,distmat, alpha = 0.2)
```

cutree() is used to deive the cluster object

```{R}
groups <- as.factor(cutree(clustG,k=6))
```

join back the group list with shan_sf polygon feature data frame

```{r}
shan_sf_Gcluster <- cbind(shan_sf,as.matrix(groups))%>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

plot the map

```{r}
qtm(shan_sf_Gcluster,"CLUSTER")
```

## 7.0 Visual Interpretation of Clusters

### 7.1 Visualising individual clustering variable

```{r}
ggplot(data = shan_sf_ngeo_cluster,
       aes(x = CLUSTER, y=RADIO_PR))+
  geom_boxplot()
```

### 7.2 Multivariate Visualisation

```{r}
ggparcoord(data = shan_sf_ngeo_cluster,
           columns = c(17:21),
           scale = "globalminmax",
           alphaLines = 0.2,
           boxplot = TRUE,
           title = "Multiple Parallel Coordinates Plots of ICT Variables by Cluster")+facet_grid(~CLUSTER)+
  theme(axis.title.x = element_text(angle = 30))
```

The parallel coordinate plot above reveals that households in Cluster 4 townships tend to own the highest number of TV and mobile-phone. On the other hand, households in Cluster 5 tends to own the lowest of all the five ICT. the scale argument of ggparcoor() provide several methods to scale the clustering variables. They are:

-   std: univariately, subtract mean and divide by standard deviation.

-   robust: univariately, subtract median and divide by median absolute deviation.

-   uniminmax: univariately, scale so the minimum of the variable is zero, and the maximum is one.

-    globalminmax: no scaling is done; the range of the graphs is defined by the global minimum and the global maximum.

-   center: use uniminmax to standardize vertical height, then center each variable at a value specified by the scaleSummary param.

-    centerObs: use uniminmax to standardize vertical height, then center each variable at the value of the observation specified by the centerObsID param

group_by() and summarise() of dplyr are used to derive mean values of the clustering variables.
```{r}
shan_sf_ngeo_cluster %>% 
  st_set_geometry(NULL) %>%
  group_by(CLUSTER) %>%
  summarise(mean_RADIO_PR = mean(RADIO_PR),
            mean_TV_PR = mean(TV_PR),
            mean_LLPHONE_PR = mean(LLPHONE_PR),
            mean_MPHONE_PR = mean(MPHONE_PR),
            mean_COMPUTER_PR = mean(COMPUTER_PR))
```
